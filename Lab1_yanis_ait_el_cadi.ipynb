{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Lab 1 - Yanis AÃ¯t El Cadi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('C:/Users/boite/Desktop/2A/SD/SD-TSIA211/TP1/data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "\n",
    "# Constructing matrices for the test set\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]\n",
    "\n",
    "\n",
    "# Loading raw data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('C:/Users/boite/Desktop/2A/SD/SD-TSIA211/TP1/Raw_Dataset_May.csv')\n",
    "\n",
    "def name_to_subcategory_and_details(col_name):\n",
    "    if np.isreal(col_name):\n",
    "        col_name = names[col_name]\n",
    "    indices = np.nonzero((data['NAME'] == col_name).values)[0]\n",
    "    if len(indices) > 0:\n",
    "        subcategory = data['SUBCATEGORY'].iloc[[indices[0]]].values[0]\n",
    "        details = data['DETAILS'].iloc[[indices[0]]].values[0]\n",
    "        return subcategory, details\n",
    "    else:\n",
    "        print('unknown name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "if \\space Aw = b \\space then \\space y(t) = b_t = (Aw)_t = \\tilde{x(t)}^T w_1 + w_0 - y(t) \\tilde{x(t)}^T w_2 \\newline\n",
    "y(t)(\\tilde{x(t)}^T w_2 + 1) =  \\tilde{x(t)}^T w_1 + w_0 \\newline\n",
    "y(t) = \\frac {\\tilde{x(t)}^T w_1 + w_0} {\\tilde{x(t)}^T w_2 + 1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boite\\AppData\\Local\\Temp\\ipykernel_17012\\299647012.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  W = np.linalg.lstsq(A,b)[0]\n"
     ]
    }
   ],
   "source": [
    "W = np.linalg.lstsq(A,b)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error is equal to 1.47\n"
     ]
    }
   ],
   "source": [
    "MSE = np.linalg.norm(np.dot(A_test,W)-b_test)/len(A_test)\n",
    "print(\"The mean squared error is equal to\",round(MSE,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error after regularization is equal to 0.91\n",
      "MSE after regularization is better comapred to the unregularized one ! ( 0.91 < 1.47 )\n"
     ]
    }
   ],
   "source": [
    "# L2 regularization strength\n",
    "lambda_value = 100\n",
    "\n",
    "# Augment the input matrix with the regularization term\n",
    "A_reg = np.concatenate([A, np.sqrt(lambda_value) * np.eye(A.shape[1])], axis=0)\n",
    "\n",
    "# Augment the target values with zeros\n",
    "b_reg = np.concatenate([b, np.zeros(A.shape[1])])\n",
    "\n",
    "# Get W\n",
    "W_l2 = np.linalg.lstsq(A_reg, b_reg, rcond=None)[0]\n",
    "\n",
    "MSE_l2 = np.linalg.norm(np.dot(A_test,W_l2)-b_test)/len(A_test)\n",
    "print(\"The mean squared error after regularization is equal to\",round(MSE_l2,2))\n",
    "\n",
    "if MSE_l2 < MSE :\n",
    "    print(\"MSE after regularization is better comapred to the unregularized one ! (\",round(MSE_l2,2),\"<\",round(MSE,2),\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\nabla f_1(w) = A^T (Aw - b) + \\lambda w $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x) - \\nabla f(y) = (A^TA+100)(x-y) \\newline\n",
    "|| \\nabla f(x) - \\nabla f(y) || <= ||(A^TA+100)||||(x-y)|| \\newline\n",
    "Donc \\space \\gamma <= \\frac 2 {||A^TA+100||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91164 iterations\n",
      "The mean squared error with the gradient descent is equal to 0.92\n"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "\n",
    "def f1(w):\n",
    "    return 1/2 * np.linalg.norm(np.dot(A,w)-b)**2 + 50*np.linalg.norm(w)**2\n",
    "\n",
    "def compute_gradient(w):\n",
    "    error_term = np.dot(A, w) - b\n",
    "    gradient = np.dot(A.T, error_term) + 100 * w\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(initial_matrix, step_size):\n",
    "\n",
    "    current_matrix = deepcopy(initial_matrix)\n",
    "    nb_iterations = 0\n",
    "\n",
    "    while np.linalg.norm(compute_gradient(current_matrix)) > 1:\n",
    "\n",
    "        # Compute the gradient of the matrix function\n",
    "        grad = compute_gradient(current_matrix)\n",
    "\n",
    "        # Update the matrix in the opposite direction of the gradient\n",
    "        current_matrix -= step_size * grad\n",
    "\n",
    "        nb_iterations += 1\n",
    "\n",
    "        # print(np.linalg.norm(compute_gradient(current_matrix)))\n",
    "        # print(grad)\n",
    "\n",
    "    return current_matrix,nb_iterations\n",
    "\n",
    "lipschitz_constant = np.linalg.norm(np.dot(A.T,A) + 100)\n",
    "step_size = 2/lipschitz_constant\n",
    "\n",
    "W_grad_desc,n_iter = gradient_descent(W, step_size)\n",
    "\n",
    "print(n_iter,\"iterations\")\n",
    "\n",
    "MSE_grad_desc = np.linalg.norm(np.dot(A_test,W_grad_desc)-b_test)/len(A_test)\n",
    "print(\"The mean squared error with the gradient descent is equal to\",round(MSE_grad_desc,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a MSE really close to the one using the np.linalg.lstsq (0.92 compared to 0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conjugate_gradient_method(initial_matrix,tolerance=1e-6,max_iter=100):\n",
    "#     x = initial_matrix\n",
    "#     r = b - np.dot(A, x)\n",
    "#     p = r\n",
    "#     rsold = np.dot(r, r)\n",
    "\n",
    "#     for i in range(max_iter):\n",
    "#         Ap = np.dot(A, p)\n",
    "#         alpha = rsold / np.dot(p, Ap)\n",
    "#         x = x + alpha * p\n",
    "#         r = r - alpha * Ap\n",
    "#         rsnew = np.dot(r, r)\n",
    "\n",
    "#         if np.sqrt(rsnew) < tolerance:\n",
    "#             break\n",
    "\n",
    "#         beta = rsnew / rsold\n",
    "#         p = r + beta * p\n",
    "#         rsold = rsnew\n",
    "\n",
    "#     return x\n",
    "\n",
    "# W_conj_grad = conjugate_gradient_method(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_2(w) = \\frac 1 2 ||Aw-b||^2 \\space \\space\\space \\space \\nabla f_2 (w) = A^T(Aw-b) \\newline\n",
    "g_2(w) = \\frac \\lambda 2 ||w||_1 \\space and \\space prox_{g_2}(w) = argmin_u( g_2(u) + \\frac 1 2 ||w-u||^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'OptimizeResult'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m---> 26\u001b[0m W_prox_grad \u001b[38;5;241m=\u001b[39m \u001b[43mproximal_gradient_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[144], line 17\u001b[0m, in \u001b[0;36mproximal_gradient_method\u001b[1;34m(x0, max_iter, tol)\u001b[0m\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Compute the gradient of the smooth part\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Update the parameter using the proximal operator\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m prox_1Lg2(x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mlipschitz_constant \u001b[38;5;241m*\u001b[39m gradient)\n",
      "Cell \u001b[1;32mIn[133], line 7\u001b[0m, in \u001b[0;36mcompute_gradient\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradient\u001b[39m(w):\n\u001b[1;32m----> 7\u001b[0m     error_term \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m b\n\u001b[0;32m      8\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(A\u001b[38;5;241m.\u001b[39mT, error_term) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m w\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'OptimizeResult'"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, rosen, rosen_der\n",
    "\n",
    "def g2(w):\n",
    "    return 50 * np.max(np.abs(w))\n",
    "\n",
    "def prox_1Lg2(w):\n",
    "    def firstargmin(u):\n",
    "        return 1/lipschitz_constant * g2(w) + 1/2*np.linalg.norm(w-u)**2\n",
    "    return minimize(firstargmin,W)\n",
    "\n",
    "def proximal_gradient_method(x0, max_iter=100, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        \n",
    "        # Compute the gradient of the smooth part\n",
    "        gradient = compute_gradient(x)\n",
    "        \n",
    "        # Update the parameter using the proximal operator\n",
    "        x = prox_1Lg2(x - 1/lipschitz_constant * gradient)\n",
    "        \n",
    "        # Check for convergence\n",
    "\n",
    "    return x\n",
    "\n",
    "W_prox_grad = proximal_gradient_method(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92782104,  8.30937124, -3.6727039 , ...,  1.98059531,\n",
       "       -3.05717441, -1.18861393])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "compute_gradient(W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
